{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2025 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Gemma deployment to GKE using vLLM on GPU\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading and deploying Gemma, open models from Google DeepMind. In this guide we specifically use L4 GPUs but this guide should also work for A100(40 GB), A100(80 GB), H100(80 GB) GPUs.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Deploy and run inference for serving Gemma with vLLM on GPUs.\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\n",
        "\n",
        "Before you use GPUs in GKE, we recommend that you complete the following learning path:\n",
        "\n",
        "Learn about [current GPU version availability](https://cloud.google.com/compute/docs/gpus)\n",
        "\n",
        "Learn about [GPUs in GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXKO7CLz3U8x"
      },
      "source": [
        "### Pre requisites\n",
        "- Install Google Cloud CLI\n",
        "https://cloud.google.com/sdk/docs/install-sdk\n",
        "- Create a .env file with the following values\n",
        "\n",
        "\n",
        "```\n",
        "PROJECT_ID = \"<your project id>\"\n",
        "REGION = \"us-central1\"\n",
        "HF_TOKEN = \"<your hugging face token>\"\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Create a GKE cluster and node pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 8467,
          "status": "ok",
          "timestamp": 1754332525066,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "jGogmQvl2wZ-",
        "outputId": "88ac1f97-943b-4d70-e89c-ac46823636c3"
      },
      "outputs": [],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSm5s9I_5N40"
      },
      "source": [
        "**Restart the runtime session**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "executionInfo": {
          "elapsed": 1282,
          "status": "ok",
          "timestamp": 1754332535008,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "2Q_hqg0X2svT"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "load_dotenv()  # This loads the variables from .env into the environment\n",
        "\n",
        "PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
        "REGION = os.getenv(\"REGION\")\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import datetime\n",
        "import os\n",
        "\n",
        "# The HuggingFace token used to download models.\n",
        "\n",
        "assert HF_TOKEN, \"Set Hugging Face access token in `HF_TOKEN`.\"\n",
        "\n",
        "\n",
        "# Set up gcloud.\n",
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet\n",
        "\n",
        "# Create a unique cluster name to avoid conflicts.\n",
        "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
        "CLUSTER_NAME=f\"gke-gemma-cluster-test2\"\n",
        "\n",
        "print(f\"Creating cluster: {CLUSTER_NAME}\")\n",
        "\n",
        "! gcloud container clusters create {CLUSTER_NAME} \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --region={REGION} \\\n",
        "    --subnetwork=\"default\" \\\n",
        "    --workload-pool={PROJECT_ID}.svc.id.goog \\\n",
        "    --release-channel=rapid \\\n",
        "    --num-nodes=4 \\\n",
        "    --enable-shielded-nodes \\\n",
        "    --shielded-secure-boot\\\n",
        "    --shielded-integrity-monitoring\n",
        "\n",
        "! gcloud container node-pools create gpupool \\\n",
        "    --accelerator=type=nvidia-l4,count=2,gpu-driver-version=latest \\\n",
        "    --project={PROJECT_ID} \\\n",
        "    --location={REGION} \\\n",
        "    --node-locations={REGION}-a \\\n",
        "    --cluster={CLUSTER_NAME} \\\n",
        "    --machine-type=g2-standard-24 \\\n",
        "    --num-nodes=1 \\\n",
        "    --shielded-secure-boot \\\n",
        "    --shielded-integrity-monitoring\n",
        "\n",
        "! gcloud container clusters get-credentials {CLUSTER_NAME} --location {REGION}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n06Ni3y9zGzC"
      },
      "source": [
        "### Create a Kubernetes secret for Hugging Face credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eFs211Q4V4s"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters get-credentials gke-gemma-cluster-test --location {REGION}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1338,
          "status": "ok",
          "timestamp": 1754332580740,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": 420
        },
        "id": "Q1nW_fsPzEYE",
        "outputId": "c16b2633-62a5-4906-ae5e-2ade12852c36"
      },
      "outputs": [],
      "source": [
        "# Create Kubernetes secret for Hugging Face credentials\n",
        "! kubectl create secret generic hf-secret \\\n",
        "    --from-literal=hf_api_token={HF_TOKEN} \\\n",
        "    --dry-run=client -o yaml > hf-secret.yaml\n",
        "\n",
        "! kubectl apply -f hf-secret.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6psJZY_zUDgj",
        "outputId": "1c206c0e-fac0-4ba8-c8ca-08af582830fc"
      },
      "outputs": [],
      "source": [
        "# @title Deploy Gemma3\n",
        "\n",
        "# @markdown This section deploys Gemma.\n",
        "\n",
        "# @markdown Select one of the following model version and size options:\n",
        "\n",
        "# The size of the model to launch\n",
        "\n",
        "\n",
        "K8S_YAML=f\"\"\"apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: vllm-gemma-deployment\n",
        "spec:\n",
        "  replicas: 1\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: gemma-server\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: gemma-server\n",
        "        ai.gke.io/model: gemma-3-1b-it\n",
        "        ai.gke.io/inference-server: vllm\n",
        "        examples.ai.gke.io/source: user-guide\n",
        "    spec:\n",
        "      containers:\n",
        "      - name: inference-server\n",
        "        image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250312_0916_RC01\n",
        "        resources:\n",
        "          requests:\n",
        "            cpu: \"2\"\n",
        "            memory: \"10Gi\"\n",
        "            ephemeral-storage: \"10Gi\"\n",
        "            nvidia.com/gpu: \"1\"\n",
        "          limits:\n",
        "            cpu: \"2\"\n",
        "            memory: \"10Gi\"\n",
        "            ephemeral-storage: \"10Gi\"\n",
        "            nvidia.com/gpu: \"1\"\n",
        "        command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n",
        "        args:\n",
        "        - --model=$(MODEL_ID)\n",
        "        - --tensor-parallel-size=1\n",
        "        - --host=0.0.0.0\n",
        "        - --port=8000\n",
        "        - --enable-auto-tool-choice\n",
        "        - --tool-call-parser=pythonic\n",
        "        env:\n",
        "        - name: MODEL_ID\n",
        "          value: google/gemma-3-1b-it\n",
        "        - name: HUGGING_FACE_HUB_TOKEN\n",
        "          valueFrom:\n",
        "            secretKeyRef:\n",
        "              name: hf-secret\n",
        "              key: hf_api_token\n",
        "        volumeMounts:\n",
        "        - mountPath: /dev/shm\n",
        "          name: dshm\n",
        "      volumes:\n",
        "      - name: dshm\n",
        "        emptyDir:\n",
        "            medium: Memory\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-accelerator: nvidia-l4\n",
        "        cloud.google.com/gke-gpu-driver-version: latest\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: llm-service\n",
        "spec:\n",
        "  selector:\n",
        "    app: gemma-server\n",
        "  type: ClusterIP\n",
        "  ports:\n",
        "    - protocol: TCP\n",
        "      port: 8000\n",
        "      targetPort: 8000\n",
        "\n",
        "---\n",
        "# 3. INGRESS: Creates a shared, external load balancer that routes requests\n",
        "#    to your service based on the URL path.\n",
        "apiVersion: networking.k8s.io/v1\n",
        "kind: Ingress\n",
        "metadata:\n",
        "  name: llm-ingress\n",
        "spec:\n",
        "  rules:\n",
        "  - http:\n",
        "      paths:\n",
        "      - path: /gemma\n",
        "        pathType: Prefix\n",
        "        backend:\n",
        "          service:\n",
        "            name: llm-service\n",
        "            port:\n",
        "              number: 8000\n",
        "\"\"\"\n",
        "\n",
        "with open(\"vllm-3-1b-it.yaml\", \"w\") as f:\n",
        "    f.write(K8S_YAML)\n",
        "\n",
        "! kubectl apply -f vllm-3-1b-it.yaml\n",
        "\n",
        "# Wait for container to be created.\n",
        "import time\n",
        "\n",
        "print(\"Waiting for container to be created...\\n\")\n",
        "while True:\n",
        "    shell_output = ! kubectl get pod\n",
        "    container_status = \"\\n\".join(shell_output)\n",
        "    if \"1/1\" in container_status:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "print(container_status)\n",
        "\n",
        "# Wait for downloading artifacts.\n",
        "print(\"\\nDownloading artifacts...\")\n",
        "while True:\n",
        "    shell_output = ! kubectl logs -l app=gemma-server\n",
        "    logs = \"\\n\".join(shell_output)\n",
        "    if \"Connected\" in logs:\n",
        "        break\n",
        "    time.sleep(5)\n",
        "\n",
        "print(\"Server is up and running.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLG4OLSawOJ3"
      },
      "source": [
        "### Follow this to server the model\n",
        "\n",
        "1. Connect the cluster\n",
        "Run in Cloud Shell\n",
        "```\n",
        "gcloud container clusters get-credentials gke-gemma-cluster-test2 --region us-central1 --project dw-genai-dev\n",
        "```\n",
        "\n",
        "https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm#serve-model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbRmgoOZF6es"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "# @markdown  Delete the experiment models and endpoints to recycle the resources\n",
        "# @markdown  and avoid unnecessary continouous charges that may incur.\n",
        "\n",
        "! kubectl delete deployments tgi-gemma-deployment\n",
        "! kubectl delete services llm-service\n",
        "! kubectl delete secrets hf-secret\n",
        "\n",
        "DELETE_CLUSTER = False # @param {type: \"boolean\"}\n",
        "\n",
        "if DELETE_CLUSTER:\n",
        "  ! gcloud container clusters delete {CLUSTER_NAME} \\\n",
        "    --region={REGION} \\\n",
        "    --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "deploy_gemma3_vllm_on_gke.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
